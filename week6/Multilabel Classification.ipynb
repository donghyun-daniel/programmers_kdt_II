{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X, _y = mnist[\"data\"], mnist[\"target\"]\n",
    "_y = _y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = _X[:60000], _X[60000:], _y[:60000], _y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_large = (y_train >= 7) # 목표값이 7 이상인 경우\n",
    "y_train_odd = (y_train % 2 == 1) # 목표가 홀수인 경우\n",
    "#만약 7인 경우 두개의 레이블이 모두 1이 될 수 있다 -> multilabel!\n",
    "\n",
    "y_train_multilabel = (np.c_[y_train_large, y_train_odd]).astype(np.uint8)\n",
    "\n",
    "y_test_large = (y_test >= 7)\n",
    "y_test_odd = (y_test % 2 == 1)\n",
    "y_test_multilabel = (np.c_[y_test_large, y_test_odd]).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.round(sigmoid(X @ W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    cost = - (1/N) * (np.ones((1,N)) @ (np.multiply(np.log(sigmoid(X @ W) + epsilon), T)) @ np.ones((K,1)) +\n",
    "                      np.ones((1,N)) @ (np.multiply(np.log(1 - sigmoid(X @ W) + epsilon), (1 - T))) @ np.ones((K,1)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (sigmoid(X_batch @ W) - T_batch))\n",
    "        cost_history[i] = compute_cost(X_batch, T_batch, W)\n",
    "        if i % 10 == 0:\n",
    "            print(cost_history[i][0])\n",
    "    return (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost is: 1.386254361519801 \n",
      "\n",
      "1.3672094752553152\n",
      "1.2729324608981354\n",
      "1.2149409079494098\n",
      "1.1634788672521683\n",
      "1.1302879812623792\n",
      "1.101222513849257\n",
      "1.0613157202742673\n",
      "1.0214995416145407\n",
      "0.9890170679995822\n",
      "0.9593487061817222\n",
      "0.9265516042021402\n",
      "0.90842405841321\n",
      "0.8891809964796475\n",
      "0.8893924042074142\n",
      "0.8638468400602584\n",
      "0.8416841409411498\n",
      "0.8371215350671656\n",
      "0.8237069552008781\n",
      "0.8148634370551142\n",
      "0.8089756148995629\n",
      "0.803416918907773\n",
      "0.7983371599718219\n",
      "0.788394743199465\n",
      "0.7699195398258656\n",
      "0.7561293810963721\n",
      "0.7388143905511029\n",
      "0.7206240775929735\n",
      "0.7087969115050697\n",
      "0.7162411533936897\n",
      "0.716880036343329\n",
      "0.7115248377731034\n",
      "0.7066551750925809\n",
      "0.707111626497333\n",
      "0.7070656373628549\n",
      "0.7058099871305075\n",
      "0.7016260020251317\n",
      "0.6886698704895329\n",
      "0.7000726300531518\n",
      "0.7263100898170085\n",
      "0.7575952018171506\n",
      "0.7562178606310477\n",
      "0.757369700492815\n",
      "0.7519412314150076\n",
      "0.7332797251254096\n",
      "0.7401959452432396\n",
      "0.728405275412533\n",
      "0.7228959220445785\n",
      "0.7214538207570009\n",
      "0.714993747076993\n",
      "0.7161495233616376\n",
      "0.7189684699433744\n",
      "0.7185495480307362\n",
      "0.7278514359804019\n",
      "0.7245655560664737\n",
      "0.720088868096338\n",
      "0.7053475886068368\n",
      "0.6939366259778488\n",
      "0.6961412656501728\n",
      "0.6985791810982651\n",
      "0.6951285687012052\n",
      "0.6873628929576662\n",
      "0.6913329450313652\n",
      "0.6973069102656186\n",
      "0.6822926772950049\n",
      "0.6449548081690403\n",
      "0.658680116437098\n",
      "0.6479957509373739\n",
      "0.6589496343675487\n",
      "0.6603527201186439\n",
      "0.6554749064502721\n",
      "0.6590245857079327\n",
      "0.6640968927213087\n",
      "0.6574445884646762\n",
      "0.6741857176580026\n",
      "0.6891141704332182\n",
      "0.6860889256007037\n",
      "0.6976624105997609\n",
      "0.6836545858157077\n",
      "0.6725441464679098\n",
      "0.6462586760599012\n",
      "0.6427712794740429\n",
      "0.641224450361413\n",
      "0.6281162234166783\n",
      "0.6397247599917975\n",
      "0.6371770381813482\n",
      "0.6431419929494899\n",
      "0.6406955365397637\n",
      "0.6237765284775392\n",
      "0.641953276947564\n",
      "0.6293612561319861\n",
      "0.643378752516027\n",
      "0.615879028627098\n",
      "0.6254768037566748\n",
      "0.631575813783507\n",
      "0.6317056719871943\n",
      "0.6167429909657758\n",
      "0.618309743155764\n",
      "0.6219042961988271\n",
      "0.6282912314407476\n",
      "0.5974040888303493\n",
      "0.6158771804620773\n",
      "0.6114689037787315\n",
      "0.5964341664368967\n",
      "0.6303237119882057\n",
      "0.6387669201314552\n",
      "0.6567261632992972\n",
      "0.6649102504259505\n",
      "0.6985310032184839\n",
      "0.711373835335212\n",
      "0.702767978389749\n",
      "0.693977494760704\n",
      "0.681645017954753\n",
      "0.7098307202141979\n",
      "0.7001727847021465\n",
      "0.6898179984533983\n",
      "0.6807308442897706\n",
      "0.6784473630404011\n",
      "0.6730717354558211\n",
      "0.6606201535260858\n",
      "0.6490786343727968\n",
      "0.6510850212559762\n",
      "0.655720768943107\n",
      "0.6425966473116347\n",
      "0.6503537927901049\n",
      "0.6456539556472863\n",
      "0.6474192521492133\n",
      "0.6451600648174256\n",
      "0.6599007285830005\n",
      "0.6589087477244326\n",
      "0.6520456706839924\n",
      "0.646277399181324\n",
      "0.6500530559177078\n",
      "0.6313054372618847\n",
      "0.6080909604152511\n",
      "0.6073047567566751\n",
      "0.6209693953804707\n",
      "0.6192033186707981\n",
      "0.5887175218312404\n",
      "0.6059841118501401\n",
      "0.5989463816176228\n",
      "0.5970729087722508\n",
      "0.6064347619505143\n",
      "0.5934648312312268\n",
      "0.589138781905709\n",
      "0.5933728325731817\n",
      "0.6021796377774737\n",
      "0.6319196453668507\n",
      "0.6274493047689762\n",
      "0.6237533429994808\n",
      "0.6234590542805912\n",
      "0.6291519356543398\n",
      "0.6020633165672724\n",
      "0.589358985783863\n",
      "0.5827002161794945\n",
      "0.5910478791146255\n",
      "0.5919594295662933\n",
      "0.5872990454067564\n",
      "0.5861280739173362\n",
      "0.5723943550609212\n",
      "0.578941161555384\n",
      "0.5669905420873704\n",
      "0.5995952225713793\n",
      "0.6092203192903378\n",
      "0.5966704171933749\n",
      "0.5837760379558927\n",
      "0.5855176101642472\n",
      "0.5972786180934432\n",
      "0.5920550566354303\n",
      "0.6005741459532967\n",
      "0.6027787892827452\n",
      "0.6037492166789814\n",
      "0.6183110260146014\n",
      "0.6005131336741273\n",
      "0.5913397809454247\n",
      "0.5798491237470607\n",
      "0.5873413169550206\n",
      "0.5944488637479408\n",
      "0.6003224935697431\n",
      "0.5919352523232685\n",
      "0.6011195957549917\n",
      "0.5740130595718367\n",
      "0.5844522954997164\n",
      "0.5776010394558505\n",
      "0.593647771986189\n",
      "0.5928071680694185\n",
      "0.609042840058323\n",
      "0.6112302997402397\n",
      "0.6013397997293622\n",
      "0.5910508522125785\n",
      "0.5994103660361181\n",
      "0.5966802206326542\n",
      "0.6042166270809701\n",
      "0.5944356229338632\n",
      "0.5889140923664955\n",
      "0.5848857087542356\n",
      "0.5743643506591987\n",
      "0.5704302896681194\n",
      "0.5585840671426199\n",
      "0.5538365346935314\n",
      "0.5862422108010791\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "T = y_train_multilabel\n",
    "\n",
    "K = np.size(T, 1)\n",
    "M = np.size(X, 1)\n",
    "W = np.zeros((M,K))\n",
    "\n",
    "iterations = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "initial_cost = compute_cost(X, T, W)\n",
    "\n",
    "print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "(cost_history, W_optimal) = batch_gd(X, T, W, learning_rate, iterations, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8873 0.863 ]\n"
     ]
    }
   ],
   "source": [
    "## Accuracy\n",
    "X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "y_pred = predict(X_, W_optimal)\n",
    "score = sum(y_pred == y_test_multilabel)/ len(y_test_multilabel)\n",
    "\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
